%!TeX root = thesis-main.tex
\chapter{Reinforcement Learning}\label{chap:marl}

\minitoc% Creating an actual minitoc
\newcommand{\RS}{\mathcal{S}}
\newcommand{\RA}{\mathcal{A}}
\newcommand{\RP}{\mathcal{P}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\RE}{\mathbb{E}}

The concept of intelligence is as complex as it is intriguing, 
 and it has been a subject of philosophical inquiry, 
 scientific exploration, and cultural curiosity for centuries. 
 Philosophers have debated on what constitutes intelligence, 
 linking it to \emph{reason}, \emph{wisdom}, and even \emph{morality}. 
%
Despite these varied interpretations, 
 defining intelligence remains a challenge, 
 even in the field of psychology. 
%
Several standardized tests and scales attempt to measure intelligence,
 like the one developed by Alan Turing~\cite{Turing1950-TURCMA}, 
 but none manage to capture the complete essence of what it means to be ``intelligent''. 
 Intelligence is often understood as the ability to \emph{learn}, \emph{reason}, and \emph{adapt}, among other cognitive abilities.

Among the myriad of perspectives on intelligence, learning stands out as a \emph{fundamental} component. 
 From an evolutionary point of view, 
 the ability to learn is essential for survival. 
 An organism that can adapt to its environment and learn from experiences is likely to survive and reproduce. 
 In the human context, learning has been the cornerstone of development, be it mastering a language, solving complex problems, or creating art.

This notion of learning is crucial in the realm of Artificial Intelligence (AI). 
%
 If intelligence involves learning, 
 then replicating intelligence artificially would necessarily entail enabling machines to learn. 
 This hypothesis leads us to the exciting and rapidly evolving domain of \emph{machine learning}---
 a subset of AI that allows computers to learn from data, 
 rather than requiring them to be explicitly programmed for specific tasks.

Machine learning is not monolithic; 
 it encompasses various approaches and techniques that aim to make machines learn. 
 Broadly, these approaches can be categorized into supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning.
%
Particularly, the first three approaches are based on the idea of learning from data, 
 where the data is either labelled or unlabelled:
\begin{itemize}
  \item \emph{supervised learning}: this is the most straightforward approach, where a model is trained on a labelled dataset. 
  The model makes predictions or decisions based on input data and is corrected when its predictions are incorrect. Typical examples include classification and regression problems;
  \item \emph{unsupervised learning}: unlike supervised learning, this approach does not involve labelled data. 
  The machine tries to learn the patterns and the structure from the data without any supervision (e.g., clustering algorithms);
  \item \emph{semi-supervised}: A middle-ground between supervised and unsupervised learning, this approach utilizes both labelled and unlabelled data for training. The model learns to improve its predictions gradually.
\end{itemize}
Reinforcement learning sets itself apart from other methodologies by operating without the need for labelled data or supervision and through a sequential decision. 
 It employs a \emph{trial-and-error} strategy, mirroring the way humans acquire knowledge. 
 Subsequent sections will delve into the nuances of this distinctive approach, starting from single agent settings and then moving to multi-agent and many-agent systems---the focus of this thesis.
\section{Single-agent}\label{chap:rl:single}
\begin{figure}
  \includegraphics[width=\textwidth]{chapters/img/single-agent-rl.drawio.pdf}
  \caption{Overview of the \ac{rl} framework.}\label{fig:rl:overview}
\end{figure}
\Acl{rl}~\cite{sutton2018reinforcement-learning} serves as a universal framework that h
 as been inspired by the cognitive processes underlying human learning. 
 This paradigm has proven to be highly effective for addressing \emph{control problems}, 
 which are essentially tasks that require decision-making to achieve a particular outcome.
%
The core focus of \ac{rl} is on the \emph{sequential} interactions that occur between \emph{agents} 
 and an \emph{environment} (summarized in \Cref{fig:rl:overview}). 
 Agents are defined as entities capable of performing \emph{actions}, 
 while the environment constitutes everything that is external to the agents 
 and beyond their immediate control.

During each discrete time step, 
 denoted as $t$, an agent observes the current state of the environment, 
 $s_t$ (e.g., the robot position according to a GPS sensor). 
This state encapsulates the set of all observable information at that particular moment. 
The agent then proceeds to select an \emph{action} 
 (e.g., the torque to be applied to engines) 
 $a_t$ in accordance with its \emph{policy} $\pi$. 
A policy serves as a probabilistic mapping that guides the agent in choosing actions 
 based on the current state. 
 Policies can be simple lookup tables or complex neural networks.

As a result of taking this action, 
 the environment transitions to a new state $s_{t+1}$ at the next time step $t+1$. 
 Simultaneously, the agent receives a \emph{reward} $r_{t+1}$, 
 which is a quantitative measure of the efficacy of the action taken, 
 given the state of the environment.

The overarching objective of \ac{rl} is to discover an \emph{optimal} policy, 
 denoted as $\pi^*$, that aims to maximize the long-term return, or cumulative reward, $G$. 
 This is generally achieved through a \emph{trial-and-error} learning process, 
 where agents continually adapt their policies based on the rewards received.

This framework has found extensive applications in a diverse array of domains. 
 For example, \ac{rl} has been used to create advanced algorithms for video games~\cite{DBLP:journals/spm/ArulkumaranDBB17}, 
 allowing for AI agents that can outperform human players. 
% 
In robotics~\cite{DBLP:journals/ijrr/KoberBP13}, 
 \ac{rl} algorithms are enabling machines to learn complex tasks autonomously, 
 from simple object manipulation to navigation in unstructured environments. 
 It is also making significant inroads in networking, 
 particularly in routing algorithms where dynamic decision-making is crucial~\cite{DBLP:journals/comsur/LuongHGNWLK19}.

\subsection{Markov Decision Process}
This general framework is supported by \ac{mdp}, 
 a mathematical model that describes the environment evolution in sequential decision problems. 
%
A \ac{mdp} consists of a tuple $<\RS, \RA, \RP, \RR>$ in which:
\begin{itemize}
  \item $\RS$ denotes the set of states;
  \item $\RA$ is the set of actions;
  \item $\RP(s_{t + 1} | s_t, a_t)$ define the probability to reach some state $s_{t + 1}$ starting from $s_t$ and performing $a_t$ (i.e. transition probability function);
  \item $\RR(s_t, a_t, s_{t+1})$ devise a probabilistic reward function.
\end{itemize}
In \ac{mdp}, $\RP$ is \emph{memory-less}, 
 namely the next environment state depends only on the current state---
 that is the \emph{Markov property}.
%%
%Typically, in \ac{rl} problems, agents do not have access to $\RR$ or $\RP$, 
% but they can rely only on the experience $(s_t, a_t, r_t)$ sampled at a time step. 
%%
Another important concept in \ac{mdp} is the return 
 $G$ defined as the discounted sum of reward a possible future trajectory $\tau$ (i.e. a sequence of time steps):
%%
\begin{equation}
G_{t} = r_t + \gamma r_{t + 1} + \gamma^2 r_{t + 2} + \dots + \gamma^T r_{t + T} = \sum_{k = t}^T \gamma^{k-t} r_k
\end{equation}
%%
Where $0 \leq \gamma \leq 1$ is the \emph{discount factor}, 
 that is how much the future reward impacts the long-term return.
%%
Based on the value of $T$, we can distinguish between \emph{episodic} and \emph{continuous} tasks.
 The foster ones are characterized by a finite number of time steps
  (e.g., a match of chess), while the latter ones are infinite 
  (e.g., a robot that should wander in an unknown environment).
%%
\subsubsection*{Reinforcement Learning Goal}
The \ac{rl} goal can be expressed as the maximization of the \emph{expected} 
 long-term return following a policy $\pi$:
%%
\begin{equation}
J = \mathbb{E_\pi}\Big[ G_t \Big] = \RE_\pi \Big[ \sum_{k = t}^T \gamma^{t-k} r_k \Big] 
\end{equation}
%%
Particularly, in \ac{RL} we want to find the optimal policy $\pi^*$ that maximizes $J$:
%%
\begin{equation}
\pi^* = \arg \max_{\pi} J
\end{equation}
The equation essentially captures the trade-off between immediate and future rewards. 
 The agent aims to select actions based on the policy \(\pi\) 
 that will maximize this expected long-term return. 
 The discount factor \(\gamma\) allows us to model the agent's consideration 
 for future rewards and is a hyperparameter that can be tuned based on the specific problem being solved.

\subsection{Find a policy given a MDP}

$V^\pi$ is the value function that evaluates how good (or bad) a \emph{state} 
 is according to the long-term return following the policy $\pi$ (\emph{expected value}).
% 
It is defined as:
%%
\begin{equation}
V(s)^\pi = \RE_\pi \Big[ G_t | s_t = s \Big]
\end{equation}
%%
$Q^\pi$ is the corresponding value function that evaluates \emph{state-action} pairs:
%%
\begin{equation}
Q(s, a)^\pi = \RE_\pi \Big[ G_t | s_t = s, a_t = a \Big]
\end{equation}
Policies could be defined through value functions. 
 In particular, a greedy policy based on $Q$ function is the one that always chooses 
 the action with the highest value in a certain state:
\begin{equation}
\pi(s) = \arg \max_{a}(Q(s, a))
\end{equation}
\subsubsection{Dynamic programming}
\begin{figure}
  \includegraphics[width=0.3\textwidth]{chapters/img/generalized-policy-improvement.png}
  \includegraphics[width=0.65\textwidth]{chapters/img/value-iteration.png}
  \caption{General schema of policy iteration (left) and value iteration (right).}\label{fig:rl:dp}
\end{figure}
Dynamic programming is a family of algorithms that can be used to compute optimal policies 
 given a model of the environment as a \ac{mdp}.
%
In particular, the \emph{Bellman equation} is a fundamental concept in dynamic programming. 
 It is a recursive equation that decomposes the value function into two parts: 
 the immediate reward obtained from the current state and the discounted value of the future state. 
 The Bellman equation for the value function is defined as:
\begin{equation}
V(s) = \sum_{a \in \RA} \pi(a|s) \sum_{s' \in \RS} \RP(s'|s, a) \Big[ \RR(s, a, s') + \gamma V(s') \Big]
\end{equation}
%
Similarly, the Bellman equation for the $Q$ function is defined as:
\begin{equation}
Q(s, a) = \sum_{s' \in \RS} \RP(s'|s, a) \Big[ \RR(s, a, s') + \gamma \sum_{a' \in \RA} \pi(a'|s') Q(s', a') \Big]
\end{equation}
%
The Bellman equation is the basis for many algorithms that solve \ac{mdp},
two most notable are \emph{value iteration} and \emph{policy iteration} (\Cref{fig:rl:dp}).
\subsection{Value iteration}
Value iteration is an iterative algorithm used to compute the optimal value function \(V^*\) and, consequently, the optimal policy \(\pi^*\). 
 The algorithm is particularly useful when the state and action spaces are too large to solve directly through analytical methods. 
 It is based on the principle of optimality, which states that if an optimal policy \(\pi^*\) exists, then it must satisfy the Bellman optimality equation.

The algorithm starts by initializing \(V(s)\) for all states \(s\) to some arbitrary values, often zeros. 
 It then iteratively updates the value of each state \(s\) using the Bellman optimality equation until the value function converges to \(V^*\). 
 The convergence is usually checked by measuring the difference between successive value functions and comparing it against a small threshold \(\epsilon\):
\begin{equation}
V_{k+1}(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} \RP(s'|s, a) \Big[ \RR(s, a, s') + \gamma V_k(s') \Big]
\end{equation}

The \(\max\) operation ensures that the value function is updated to reflect the best possible action at each state. 
 The term \(\gamma V_k(s')\) represents the discounted future rewards, and \(R(s, a, s')\) is the immediate reward. 
 The transition probability \(P(s'|s, a)\) models the uncertainty in the environment.

After the value function has converged to \(V^*\), the optimal policy \(\pi^*\) can be extracted. The policy is determined by selecting the action that maximizes the expected return in each state, as given by:

\begin{equation}
\pi^*(s) = \arg \max_{a} \sum_{s' \in \mathcal{S}} P(s'|s, a) \Big[ R(s, a, s') + \gamma V^*(s') \Big]
\end{equation}
This policy is guaranteed to be optimal with respect to the original MDP.


\subsection{Policy Iteration}
Policy iteration is another dynamic programming algorithm used for finding the optimal policy \(\pi^*\). 
 Unlike value iteration, policy iteration consists of two main steps: 
 \emph{policy evaluation} and \emph{policy improvement}, 
 which are repeated iteratively until the policy converges to \(\pi^*\):
\begin{itemize}
  \item policy evaluation: in this step, the value function \(V^\pi\) for the current policy \(\pi\) is computed until it stabilizes. The update rule is:

  \begin{equation}
  V_{k+1}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s, a) \Big[ R(s, a, s') + \gamma V_k(s') \Big]
  \end{equation}
  \item policy improvement: after evaluating \(V^\pi\), the policy is updated to be greedy with respect to \(V^\pi\):
  \begin{equation}
  \pi'(s) = \arg \max_{a} \sum_{s' \in \mathcal{S}} P(s'|s, a) \Big[ R(s, a, s') + \gamma V^\pi(s') \Big]
  \end{equation}
\end{itemize}
The algorithm then returns to the policy evaluation step, using the new policy \(\pi'\), and continues until the policy no longer changes.
 
%The objective in value-based methods is to find the best value function (whether $Q$ or $V$), defined as:
%\begin{iequation}
%Q^* = \arg \max_{\pi}(Q^\pi(s, a)) \; \text{or} \; V^* = \arg \max_{\pi}(V^\pi(s))
%\end{iequation}
%Consequently, when we have found $Q^*$, an optimal policy is straightforwardly defined:
%\begin{equation}
%\pi^*(s) = \underset{a}{\text{argmax}}(Q^*(s, a))
%\end{equation}
\subsection{Find a policy without an MDP}
While dynamic programming methods like value iteration and policy iteration offer powerful ways to find the optimal policy \(\pi^*\), 
 they come with a significant limitation: 
 the need for a complete model of the environment. 
 Specifically, these algorithms require knowledge of the transition probability function \(\RP\) and the reward function \(\RR\). 
 In many real-world applications, these functions are either unknown or too complex to model accurately. 
 This is where \emph{model-free} algorithms come into play.
 In the following sections, we will discuss two such algorithms' family: 
 Monte Carlo methods and Temporal Difference methods.
\subsubsection{Monte Carlo methods}
Monte Carlo methods offer a way to find an optimal policy \(\pi^*\) without requiring a model of the environment. 
 These methods rely on sampling sequences of states, actions, and rewards from actual or simulated interactions with the environment. 
 By averaging these samples, the agent can estimate the value functions \(V(s)\) and \(Q(s, a)\), which can then be used to improve the policy.

The core idea is to run multiple episodes, 
 from start to finish, and then update the value estimates based on the returns observed. 
 The value of a state \(s\) or a state-action pair \((s, a)\) is estimated as the average of the returns that have followed that state or state-action pair across multiple episodes.

\begin{equation}
V(s) = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}
\end{equation}

\begin{equation}
Q(s, a) = \frac{1}{N} \sum_{i=1}^{N} G_t^{(i)}
\end{equation}

Where \(N\) is the number of times the state or state-action pair has been visited, and \(G_t^{(i)}\) is the return following the \(i\)-th visit.

Once the value functions are estimated, 
 the policy can be improved by making it greedy with respect to these estimated values. 
%
In this context, the \emph{exploration-exploitation} trade-off is crucial.
%
In fact, the agent should explore the environment to discover new states and actions that could lead to higher rewards,
  but it should also exploit the knowledge it has already acquired to maximize the expected return.
%
Monte Carlo methods are particularly useful when the state and action spaces are large, making it impractical to enumerate all possible state-action pairs. However, they do require the episodes to be finite and can be computationally expensive due to the need for multiple samples to obtain accurate estimates.

\paragraph*{Exploration-Exploration dilemma}
In these algorithms, the agent should explore the environment to discover new states and actions that could lead to higher rewards,
  but it should also exploit the knowledge it has already acquired to maximize the expected return.
  This is the so-called \emph{exploration-exploitation} trade-off.
  This is not only a matter in Monte Carlo methods, but it is a general problem in model-free algorithms, in which agent should learn a policy without a model of the environment.
%
There are several ways to address this dilemma, 
 but the most common approach is to use an \emph{$\epsilon$-greedy} policy. 
 This policy selects a random action with probability \(\epsilon\) and the greedy action with probability \(1 - \epsilon\). 
 The value of \(\epsilon\) is typically set to a small value, 
 such as 0.1 or 0.2, 
 to ensure that the agent explores the environment sufficiently.
%

\subsubsection{Temporal difference methods}
Temporal difference methods are a class of \emph{model-free} and \emph{value-based} algorithms 
 that allow the agent to learn optimal behaviour directly from its interactions with the environment, without requiring a model. 
 TD methods combine ideas from both Monte Carlo methods and dynamic programming to provide a flexible and powerful approach to reinforcement learning.

One of the simplest TD methods is \emph{TD-Learning}~\cite{DBLP:journals/ml/Sutton88}, 
 which updates the value function \(V(s)\) based on the temporal difference error \(\delta\), 
 defined as \( \delta = R(s, a, s') + \gamma V(s') - V(s) \). 
 The value function is then updated using \( V(s) \leftarrow V(s) + \alpha \delta \), where \(\alpha\) is the learning rate.

Starting from this intuition two main approaches have been developed: 
 \emph{Q-learning}~\cite{DBLP:journals/ml/WatkinsD92} and \emph{SARSA}~\cite{10.5555/3312046}. 

 \paragraph*{Q-Learning}

 Q-Learning is an off-policy TD algorithm that focuses on learning the action-value function \(Q(s, a)\). The update rule for Q-Learning is:
 \begin{equation}
 Q(s, a) \leftarrow Q(s, a) + \alpha \Big[ R(s, a, s') + \gamma \max_{a'} Q(s', a') - Q(s, a) \Big]
 \end{equation}
 The algorithm is termed ``off-policy'' because it learns the value of the optimal policy regardless of the policy being followed, 
  thanks to the \(\max_{a'}\) term in the update rule.
 
 \paragraph*{SARSA}
 SARSA (State-Action-Reward-State-Action) is an on-policy TD algorithm. 
  Unlike Q-Learning, SARSA takes into account the current policy \(\pi\) during the learning process. The update rule for SARSA is:
 \begin{equation}
 Q(s, a) \leftarrow Q(s, a) + \alpha \Big[ R(s, a, s') + \gamma Q(s', a') - Q(s, a) \Big]
 \end{equation}
 where \(a'\) is the action taken under the current policy \(\pi\).
 
 Both Q-Learning and SARSA have their own advantages and disadvantages. 
  Q-Learning tends to find the optimal policy faster but may be more sensitive to noise. SARSA, being an on-policy method, is more conservative and tends to find safer policies, especially when the policy involves some level of risk or uncertainty.

\subsection{Policy Gradient Methods}

While temporal difference methods and monte carlo methods focus on learning value functions to derive optimal policies, \emph{policy gradient} methods take a different approach. 
  They aim to directly optimize the policy \(\pi\) itself, 
  rather than first estimating value functions. 
  This is particularly useful in environments with high-dimensional action spaces or continuous action spaces, where value-based methods may struggle.
  Moreover, in this way, it is possible to learn stochastic policies,
    which are often more robust and flexible than deterministic policies.

Policy gradient methods optimize the policy by ascending the gradient of the expected return with respect to the policy parameters \(\theta\). 
Mathematically, this can be expressed as:
\begin{equation}
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
\end{equation}
where \(J(\theta)\) is the expected return when following policy \(\pi_\theta\), and \(\alpha\) is the learning rate.

One of the foundational algorithms in this category is the REINFORCE algorithm~\cite{DBLP:conf/nips/SuttonMSM99}: it is one of the earliest and most straightforward policy gradient methods. 
It estimates the gradient of the expected return by sampling trajectories from the current policy \(\pi_\theta\). 
After each episode, the algorithm adjusts the policy parameters \(\theta\) in the direction that increases the expected return.
%
The core equation for the REINFORCE algorithm is:
\begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]
\end{equation}

Here, \(\tau\) represents a trajectory sampled from the policy \(\pi_\theta\), and \(G_t\) is the return from time \(t\). 
 The term \(\nabla_\theta \log \pi_\theta(a_t | s_t)\) is the log-likelihood gradient, 
 and \(G_t\) serves as a sample estimate for how good the action \(a_t\) is in state \(s_t\).

The REINFORCE algorithm operates in an episodic setting, 
 meaning it waits until the end of each episode to update the policy. 
 This makes it well-suited for tasks where the episode termination is natural, such as games or tasks with a fixed time horizon.
 REINFORCE is particularly useful when the action space is high-dimensional or continuous, where traditional value-based methods like Q-Learning may struggle.  
 However, one drawback of REINFORCE is that it can have high variance in its updates, which can make the training process unstable. 
 Various techniques, such as using a \emph{baseline} or employing advanced variance reduction methods, have been developed to mitigate this issue.
 Particularly, the \emph{actor-critic} architecture is a popular approach that combines the advantages of both value-based and policy-based methods.
 In this paradigm, the policy is referred to as the \emph{actor}, while the value function is referred to as the \emph{critic}.
The actor is responsible for selecting actions, while the critic evaluates the actions taken by the actor.

\subsection{Approximate Solutions}

While the fundamental algorithms discussed in previous sections provide a strong theoretical foundation for model-free RL, 
 they often fall short in real-world applications. 
 The primary challenges they face are:

\begin{itemize}
  \item \textbf{Curse of dimensionality}: The state and action spaces in practical problems can be so large that enumerating them becomes computationally infeasible.
  \item \textbf{Partial observability}: In many scenarios, the agent cannot fully observe the entire state of the environment, 
  making it difficult to make optimal decisions.
\end{itemize}
To illustrate the curse of dimensionality, 
 consider the seemingly simple game of Go. 
 The game's state space consists of \(2^{170}\) possible states, 
 a number so astronomical that it exceeds computational capabilities—especially when compared to the estimated \(10^{80}\) atoms in the observable universe.

Similarly, partial observability is a pervasive issue in real-world applications. 
 For example, a self-driving car perceives its environment through sensors, 
 offering only a limited, partial view of the world. 
 This restricted perspective can significantly impact the agent's ability to make optimal decisions.

Given these challenges, approximate solutions become not just desirable but often necessary. 
 These solutions leverage function approximation techniques to estimate value functions or policies. 
 While they may sacrifice some theoretical convergence guarantees, 
 they offer a more practical approach to tackling complex, high-dimensional, and partially observable problems commonly encountered in real-world applications.
%
In contemporary applications, neural networks have emerged as the go-to function approximators due to their exceptional capability to approximate complex, 
 high-dimensional functions. 
 Particularly, the combination of Deep learning and Reinforcement learning has led to significant advancements in the field, in the so-called area of \emph{deep reinforcement learning}.
 One of the first and most influential works in this area was the Deep Q-Network (DQN) algorithm~\cite{DBLP:journals/corr/MnihKSGAWR13}, that will be discussed in the next section.
 \subsubsection{Deep Q-Networks (DQN)}

 Deep Q-Networks (DQN) represent a landmark innovation in the field of deep reinforcement learning, 
  effectively combining the strengths of Q-Learning with the function approximation capabilities of deep neural networks.
  DQN was initially designed to master a variety of Atari 2600 games and has since been adapted for various complex tasks.
 
 The core idea behind DQN is to use a neural network as a function approximate for the Q-function in Q-Learning. 
  The neural network, often referred to as the Q-network, 
  takes the environment state as input and outputs Q-values for each action.
%  
Mathematically, the Q-network aims to approximate the optimal Q-function \(Q^*(s, a)\) 
 as closely as possible.
\begin{equation}
Q(s, a; \theta) \approx Q^*(s, a)
\end{equation}
 
where \(\theta\) represents the parameters of the neural network.
 
However, directly applying neural networks to Q-Learning presents challenges, 
 primarily due to the correlation between consecutive experiences and the non-stationary nature of the data. 
 DQN addresses these issues through two key innovations:
 
\begin{itemize}
  \item \textbf{Experience replay}: DQN stores past experiences \((s, a, r, s')\) in a replay buffer and samples mini-batches randomly during training. 
  This decorrelates the data and leads to more stable training.
  \item \textbf{Target network}: DQN introduces a separate, 
  slowly-updated target network to calculate the target Q-values, 
  reducing the overestimation bias and improving stability.
\end{itemize}
 
The Q-value update equation in DQN is:
\begin{equation}
Q(s, a; \theta) \leftarrow Q(s, a; \theta) + \alpha \Big[ r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \Big]
\end{equation}
Where \(\theta^-\) are the parameters of the target network.
\subsection{Wrap up}
\begin{figure}
  \includegraphics[width=\textwidth]{chapters/img/rl-overview.drawio.pdf}
  \caption{Overview of the \ac{rl} algorithms.}\label{fig:rl:overview}
\end{figure}
In conclusion, 
 this section has provided a comprehensive overview of RL, 
 beginning with the formulation of the problem and the underlying mathematical framework. We also explored various analytical solutions and key algorithms associated with RL. 

Reinforcement learning algorithms can be categorized along multiple dimensions (summarized in \Cref{fig:rl:overview}). 
 One primary distinction is between \emph{model-free} and \emph{model-based} algorithms. 
 In the former, there is no need for a model of the environment, allowing the algorithm to learn directly through interaction. 
 In contrast, model-based algorithms require an MDP to compute the optimal policy.
%
Another important categorization is whether an algorithm is \emph{on-policy} or \emph{off-policy}. 
 On-policy algorithms optimize the same policy that is used for exploration, 
 whereas off-policy algorithms utilize two separate policies during the learning process, commonly referred to as the behaviour and target policies.
%
RL algorithms can be broadly divided based on what they aim to learn. 
 The primary categories here are value-based and policy gradient methods, 
 although hybrid approaches also exist that combine elements of both, like actor critic.
%
Lastly, RL algorithms can be categorized based on whether they use a \emph{tabular} or \emph{approximate} approach. 
 Tabular methods store the value function in a table, 
 while approximate methods leverage function approximation techniques, 
 such as neural networks, to estimate the value function.

This overview serves as a basis for the multi-agent and many-agent RL algorithms,
 because most of the algorithms that we will discuss in the following sections are based on the concepts presented here.

\section{Multi-agent}
\begin{figure}
\includegraphics[width=\textwidth]{chapters/img/multi-agent-rl.drawio.pdf}
\caption{Overview of the \ac{marl} framework.}\label{fig:marl:overview}
\end{figure}
In the evolving landscape of \ac{rl}, the concept of \acf{marl} (\Cref{fig:marl:overview}) stands as a natural extension of the foundational \ac{rl} principles. 
 While traditional \ac{rl} generally focuses on the interactions between a single agent and an environment, 
 \ac{marl} broadens the scope to include multiple agents, 
 each with their own objectives, policies, and decision-making processes.

The basic setting in \ac{marl} comprises multiple agents interacting either \emph{cooperatively}, \emph{competitively}, or in a \emph{mixed} fashion within a shared environment---more details in the following sections. 
 Each agent $i$ continues to observe its own state $s_{t}^{i}$, select actions $a_{t}^{i}$ according to its policy $\pi^{i}$, and receive rewards $r_{t+1}^{i}$.
 However, in \ac{marl}, 
 an agent's actions can directly or indirectly influence the states and rewards of other agents, 
 thereby increasing the complexity of the learning problem.
The key challenge in \ac{marl} 
 is to develop robust algorithms that enable agents to learn optimal policies in these complex, often \emph{non-stationary}, environments. 
 Classic \ac{rl} algorithms often require modifications to accommodate the multi-agent setting. 
 For instance, the concept of a joint action space, a state space extended to multiple agents, and a composite reward function are essential considerations.
\subsection{Stochastic games}
The formalization of MARL typically extends the standard Markov Decision Process (MDP) 
 framework to account for multiple agent. 
 One of the most straightforward extensions is the Markov Game, also called Stochastic games. 
 In this formalization, 
each agent has its own state, action, and reward function, 
and the joint actions of all agents determine the transition dynamics and rewards: 
\begin{equation}
S = \langle \mathcal{N}, \mathcal{S}, \mathcal{A}_1, \ldots, \mathcal{A}_N, \mathcal{P}, \mathcal{R}_1, \ldots, \mathcal{R}_N \rangle
\end{equation}
Where:
\begin{itemize}
    \item $\mathcal{N}$ is the number of agents. With $N=1$ we have a single-agent setting, while with $N >> 2$ we have a many-agent setting.
    \item $\mathcal{S}$ is the environment state space. The environment is then considered to be fully observable. 
    \item $\mathcal{A}_i$ is the action space of agent $i$. We denote the joint action space as $\mathbb{A} = \mathcal{A}_1 \times \ldots \times \mathcal{A}_N$.
    \item $\mathcal{P}: \mathcal{S} \times \mathbb{A} \rightarrow \Delta(\mathbb{S})$ is the joint transition probability function. 
    For each time step $t$, $\mathcal{P}$ is a function of the joint action $a_t \in \mathbb{A}$ and the current state $s_t \in \mathcal{S}$, 
    and returns the probability of transitioning to the next state $s_{t+1} \in \mathcal{S}$.
    \item $\mathcal{R}_i: \mathbb{S} \times \mathbb{A} \times \mathbb{S} \rightarrow \mathbb{R}: $ is the reward function for agent $i$. 
    The joint reward function is defined as $\mathbb{R} = \mathcal{R}_1 + \ldots + \mathcal{R}_N$.
\end{itemize}
The game can be described sequentially as follows:
\begin{enumerate}
    \item At each time step $t$, each agent $i$ observes the current state $s_t^i$.
    \item Each agent $i$ selects an action $a_t^i$ according to its policy $\pi^i$.
    \item The joint action $a_t = (a_t^1, \ldots, a_t^N)$ is executed, and the environment transitions to the next state $s_{t+1}$ according to the transition probability function $\mathcal{P}$.
    \item Each agent $i$ receives a reward $r_{t+1}^i$ according to the reward function $\mathcal{R}_i$.
\end{enumerate}
It is important to note that in these games, actions from all agents are executed simultaneously, and each agent's reward is a function of the joint action taken by all agents. In many real-world scenarios, assuming the availability of a global system state is impractical. As such, an often-employed extension to the traditional Stochastic Game model is the \emph{Partially Observable Stochastic Game} (POSG). This extension can be formally represented as follows:
\begin{equation}
\text{POSG} = \langle \mathcal{N}, \mathcal{S}, \mathcal{A}_1, \ldots, \mathcal{A}_N, \mathcal{P}, \mathcal{R}_1, \ldots, \mathcal{R}_N, \mathcal{O}_1, \ldots, \mathcal{O}_N, \mathcal{O} \rangle
\end{equation}
Here, 
 the primary divergence from the conventional Stochastic Game 
 is the introduction of observation functions \(\mathcal{O}_i: \mathcal{S} \rightarrow \mathcal{O}_i\), which map the environment state to the observation space of agent \(i\). 
 Consequently, the agents' policies are now parameterized by their respective observation spaces \(\mathcal{O}_i\) rather than the global state space \(\mathcal{S}\). 
 This alteration significantly influences both the algorithmic approaches and the complexities involved in solving POSGs.
\subsection{Taxonomies}
\begin{figure}
  \includegraphics[width=\textwidth]{chapters/img/marl-taxonomy.drawio.pdf}
  \caption{Overview of the \ac{marl} taxonomies.}\label{fig:marl:taxonomy}
\end{figure}
In single-agent settings, the focus of learning configurations is primarily on algorithmic aspects, 
 such as whether an approach is value-based or relies on policy search, or whether it operates on-policy or off-policy. 
%
However, the landscape becomes more complex when multiple agents are involved . 
In such multi-agent settings, 
 several dimensions must be considered to appropriately classify algorithms and approaches. 
Various surveys have attempted to categorize the diverse classes of algorithms specifically designed for multi-agent scenarios. 
Each of these surveys emphasizes a unique characteristic or aspect of the problem under consideration. 
%
This section endeavours to outline various dimensions along which multi-agent algorithms can be categorized (\Cref{fig:marl:taxonomy}). The objective is to provide a framework for situating the algorithms that will be discussed in subsequent sections, particularly those that pertain to the \emph{many-agent} perspective.
Note that the following taxonomies are not mutually exclusive, 
 and many algorithms can be classified along multiple dimensions.

\subsubsection{Learning Paradigms}
\begin{figure}
  \includegraphics[width=\textwidth]{chapters/img/marl-policy-kind.drawio.pdf}
  \caption{Overview of the \ac{marl} learning paradigms.}\label{fig:marl:policy-kind}
\end{figure}
\sloppy
One possible categorization of multi-agent algorithms is based on the learning paradigm (summarized in \Cref{fig:marl:policy-kind}). 
 In this context, the learning paradigm refers to the way in which agents learn their policies. 
The field of multi-agent learning has historically been dominated by two primary approaches: 
 \emph{independent learning} and \emph{centralized learning}. 
 In independent learning, 
 each agent learns its own policy concurrently and independently of the other agents in the environment~\cite{tan1993multi}. 
 In contrast, centralized learning involves a single learning entity that constructs a joint policy to control all agents.

Each of these approaches presents unique advantages and challenges. 
 Independent learning excels in scalability but often results in unstable learning dynamics due to the non-stationarity of the environment. 
 This non-stationarity stems from the concurrent learning of multiple agents, making it challenging to learn effective policies. 
%
On the other hand, centralized learning benefits from environmental stability owing to the global view it maintains. 
 However, it suffers from scalability issues due to the exponential growth of the state and action spaces as the number of agents increases.
%
To illustrate, consider a system comprising \( N \) agents, 
 each capable of taking \( M \) actions. 
 In a centralized framework, the number of possible joint actions would be \( M^N \). 
 For instance, with \( N=100 \) and \( M=2 \), the number of joint actions would be an astronomical \( 2^{100} \).

Recognizing the limitations of both independent and centralized learning paradigms, 
 recent research has proposed a compromise: Centralized Training with Decentralized Execution (CTDE)~\cite{DBLP:conf/nips/LoweWTHAM17}. 
 In this approach, agents are trained using a \emph{global} view of the environment to learn a distributed policy. 
 Indeed, during execution, each agent relies solely on its own local observations to select actions. 
 This hybrid method is particularly advantageous in partially observable environments: 
 agents can learn a policy influenced by global states during the training phase while requiring only local observations for decision-making during execution.

\subsubsection{Task type}
Aside the way in which the agent learns, another important distinction involve \emph{what} the agent should learn.
In fact, the task type can be \emph{cooperative}, \emph{competitive}, or \emph{mixed}.
\paragraph*{Cooperative}
In cooperative tasks, agents are geared towards a common objective, 
 aiming to maximize a shared reward. 
 Formally, this is represented as a unified reward function for all agents, given by:
\begin{equation}
\mathcal{R}_0 = \mathcal{R}_1 = \ldots = \mathcal{R}_N = \mathcal{R}
\end{equation}
Here, the objectives of all agents are perfectly aligned, 
 necessitating coordination and collaboration among them to achieve the shared goal. 
 Such cooperative scenarios are often encountered in the domain of \ac{cpsw}, 
 where inter-agent coordination is crucial for realizing collective outcomes effectively and efficiently.
\paragraph*{Competitive}
In competitive tasks, agents operate under divergent objectives, 
 and the reward function is usually tailored to each agent's individual performance. 
This can be formally expressed in two-player games as:
\begin{equation}
\mathcal{R}_0 = -\mathcal{R}_1
\end{equation}
In this framework, the agents have conflicting goals, 
 creating a competitive landscape where each agent aims to maximize its own reward. 
\paragraph*{Mixed}
In mixed tasks, agents operate under both cooperative and competitive dynamics. 
 This is often encountered in real-world scenarios, 
 where agents may have both shared and divergent objectives. 
 This is the most general setting, and it is often the most challenging to solve.

\subsubsection{Policy type}
Another crucial aspect to explore is the nature of the policy that an agent ought to learn. 
Although numerous classification of policies exist---ranging from deterministic to stochastic, centralized to decentralized, and joint to individual---this thesis places special focus on distinguishing between \emph{homogeneous} and \emph{heterogeneous} policy family.

\paragraph*{Homogeneous Policies}
In the context of homogeneous policies, 
 all agents \emph{share} a common policy. 
 The learning process aims to identify a single, unified policy applicable to all agents. 
 This is particularly prevalent in cooperative tasks where agents must collaborate to achieve a collective objective. 
 Utilizing a single policy across all agents often simplifies the learning process and enhances coordination among the agents.
 Moreover, is particularly suitable for scenarios where agents are \emph{interchangeable} and \emph{indistinguishable}, making it possible to use the same policy in different agent population.

\paragraph*{Heterogeneous Policies}
Conversely, in the realm of heterogeneous policies, 
 each agent possesses its own distinct policy. 
 The learning algorithm is tasked with discovering a unique policy tailored to each individual agent. 
 This approach, even if it can be also used in the context of cooperative tasks, 
 is especially useful in competitive environments, 
 where agents have disparate objectives and strive to optimize their individual rewards. 
 Employing unique policies for each agent allows for the development of diverse strategies and behaviours, accommodating the complex dynamics of competitive settings.
\subsubsection{Communication}
Another important dimension to consider is whether agents are allowed to communicate with each other. 
 In many real-world scenarios, agents are capable of exchanging information with other agents in the environment. 
 This communication can be \emph{explicit}, 
 such as through the exchange of messages, or \emph{implicit}, 
 such as through the observation of other agents' actions. 

In contrast, in other scenarios, agents are not allowed to communicate with each other. 
 This is often the case in competitive settings, 
 where agents are not permitted to share information with each other. 
 This restriction is often imposed to increase the complexity of the problem and to encourage the development of more sophisticated strategies.

\subsection{Solutions for MARL}
Addressing the challenges and nuances of \ac{marl} involves developing algorithms and frameworks that can navigate the complex landscape characterized by multi-agent interactions, various task types, and differing learning paradigms. This section delves into some of the key approaches and solutions that have shown promise in mitigating these challenges.

\subsection{Decomposition Methods}
One way to tackle the complexity in \ac{marl} is to decompose the global state and action spaces into smaller, more manageable components. Techniques like value decomposition algorithms (VDA) aim to separate the joint action-value function into individual components that can be optimized separately. This allows agents to focus on a sub-problem rather than the entire joint problem, thereby improving computational efficiency.

\begin{equation}
Q(s, a_1, \ldots, a_N) = \sum_{i=1}^{N} Q_i(s, a_i)
\end{equation}

\subsection{Multi-Agent Coordination}
For cooperative tasks, coordinating the actions of multiple agents becomes crucial. Several techniques such as multi-agent common knowledge methods and communication channels have been introduced. In these approaches, agents share a common set of signals or even develop a communication protocol to better coordinate their policies.

\subsection{Deep MARL Algorithms}
Deep learning techniques have also been adapted to the multi-agent setting to handle the challenges of high-dimensionality and partial observability. Algorithms like MADDPG and MA3C employ neural networks to approximate complex functions for policy and value estimations, enabling agents to learn in more complicated, real-world scenarios.

\subsection{Handling Non-Stationarity}
Given that one agent's policy update can affect the learning landscape of other agents, handling non-stationarity is a core challenge. Methods like regret-based dynamics and policy ensembling offer ways to manage this by allowing agents to adapt to the evolving policies of other agents.

\subsection{Centralized Training with Decentralized Execution}
Building on the concept of Centralized Training with Decentralized Execution (CTDE), this approach involves a centralized entity during the training phase that makes use of global information. Post-training, agents operate independently, making it a scalable yet effective solution.

\subsection{Ad Hoc Teamwork}
In many real-world applications, agents must work in teams where they have not been pre-coordinated. Ad hoc teamwork algorithms allow an agent to adapt its behaviour to effectively cooperate with unknown or legacy agents.

\subsection{Open Problems}
Despite significant advances, several open problems remain in \ac{marl}:

\begin{itemize}
\item \textbf{Scalability:} How to scale existing solutions to many-agent systems without losing performance?
\item \textbf{Real-world applicability:} Most algorithms are tested in simplified, synthetic environments. How well will they perform in more complex, real-world settings?
\item \textbf{Inter-agent communication:} Developing more effective and natural ways for agents to communicate remains a key challenge.
\end{itemize}

This section has provided an overview of some of the existing solutions in \ac{marl}. However, it is a rapidly evolving field, and new methods and frameworks are continually being developed to tackle the inherent challenges it presents.
\section{Many-agent}
The \ac{marl} framework, even if consider multiple agents,
  still assumes that the number of agents is small (i.e. $N \leq 2$).
  For that reason, solutions that work well in \ac{marl} settings may not be suitable for \emph{many-agent} scenarios.

In this thesis, we consider \emph{homogeneous \ac{MAARL}}~\cite{yang2021many}, 
 where the set of agents is large ($N \gg 2$) and each agent is \emph{interchangeable} and \emph{indistinguishable}.
%
This research area is relevant in the context of large-scale systems 
 where collective intelligence emerges from local and repeated interaction of simple entities, like in \ac{cpsw}.
%
In such many-agent scenarios, 
 the implementation of fully decentralized learning is often unfeasible due to the large number of learning agents, 
 which makes the system non-stationary and difficult to manage. 
%
Conversely, a centralized controller capable of coordinating 
 the entire system may not be a viable solution due to scalability concerns. 
 To address this challenge, a practical solution is the adoption of \emph{\ac{CTDE}} approach.
%
The idea is to learn a policy at simulation time when there is a collective view of the system, 
 and then at runtime use that policy but only with local observations. 
%This approach creates policies that are influenced by global information but only require local information to function at runtime. 
%
The typical approach in such cases is based on actor-critic systems~\cite{DBLP:conf/nips/LoweWTHAM17,wu2022more,song2022ctds,song2022centralized},
  where the \emph{actor} is the distributed policy (with only local information) and the critic is a neural network that takes the overall system state.
%
%In fact, the first works in this direction were discussed precisely swarm robotics, %\lukas{what does this mean `in the last field'?}
% exploring new models (e.g., swarMDP~\cite{DBLP:conf/atal/SosicKZK17}) and learning algorithms capable of extrapolating a policy representing the entire system.  \lukas{This sentence still needs rework.}
%Modern approaches, however, 
% have started to consider the use of 
% Recently, deep learning approaches have been considered 
% to synthesise robust controllers capable of generalizing to new tasks. 
%In this context, mean-field reinforcement learning~\cite{pmlr-v80-yang18d} is certainly noteworthy. 
Mean-field RL~\cite{pmlr-v80-yang18d} is one of such concrete applications of \ac{CTDE} 
 where the interactions among the population of agents are estimated by considering either the effect of a single agent and the average impact of the entire population or the influence of neighbouring agents.
%
Some known approaches using mean-field reinforcement learning include Q-mean, 
 which is an extension of Q-learning to mean-field settings~\cite{yang2018mean}, 
 and actor-critic mean-field~\cite{frikha2023actor}, which combines actor-critic algorithms with mean-field approximations. 
%
These approaches have shown promising results in various domains, such as multi-agent coordination 
 and decentralized control, and are actively being researched and developed for further applications.

\subsection{Formalization}
\subsubsection{SwarMDP}
A SwarMDP is characterized by a \emph{swarming agent} ($\mathbb{A}$) and the dynamics of the environment ($\mathbb{E}$).
Specifically, $\mathbb{A}$ is a tuple ($\mathcal{S}, \mathcal{O}, \mathcal{A}, \mathcal{R}, \pi$) where:
\begin{itemize}
  \item $\mathcal{S, O, A}$ are the set of local states, observations (or features), and actions, respectively;
  \item $\mathcal{R}: \mathcal{S} \rightarrow \mathbb{R}$ is the reward function, which is influenced by the environment;
  \item $\pi: \mathcal{O} \rightarrow \mathcal{A}$ is the policy function, which maps the observations to the actions: it could be deterministic or stochastic.
\end{itemize}
Starting from this definition, the environment $\mathbb{E}$ is defined as a tuple ($\mathcal{P}, \mathbb{A}, \mathcal{T}, \xi$), where:
\begin{itemize}
  \item $\mathcal{P}$ is the total number of agents in the systems (the agent population), which is assumed to be fixed;
  \item $\mathbb{A}$ is the defined agent prototype that rules each agent $v \in P$;
  \item $\mathcal{T}: \mathcal{S}^P \times \mathcal{A}^P \times \mathcal{S}^P \rightarrow \mathbb{R}$ is the transition  global function, which is influenced by the actions of the agents and returns a collective reward -- this is typically not known by the swarming agents;
  \item $\xi: \mathcal{S^P} \rightarrow \mathcal{O^P}$ is the global observation model of the systems.
\end{itemize}
\subsubsection{Networked Markov Decision Process}

\subsection{State-of-the-art}
In many-agent systems, the sheer number of agents makes traditional MARL techniques computationally infeasible. Recent advancements focus on scalable algorithms and techniques like mean-field theory to approximate the interactions between agents. Some notable algorithms include Mean Field Q-learning and Graph Neural Networks for many-agent systems.

\section{Gran-challenges}
